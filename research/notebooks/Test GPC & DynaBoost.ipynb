{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulag/.local/lib/python3.6/site-packages/jax/lib/xla_bridge.py:119: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import tigercontrol\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from tigercontrol.controllers import Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic Loss\n",
    "def quad_loss(x, u, Q = None, R = None):\n",
    "\tx_contrib = x.T @ x if Q is None else x.T @ Q @ x\n",
    "\tu_contrib = u.T @ u if R is None else u.T @ R @ u\n",
    "\t\n",
    "\treturn np.sum(x_contrib + u_contrib)\n",
    "\n",
    "# Policy Loss\n",
    "def policy_loss(params, determine_action, w, look_back, env, cost_fn = quad_loss):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Args:\n",
    "    \"\"\"\n",
    "    y = np.zeros((env.n, 1))\n",
    "    for h in range(look_back, 0, -1):\n",
    "        v = determine_action(params, y, w[:-h])\n",
    "        y = env.dyn(y, v) + w[-h] \n",
    "\n",
    "    # Don't update state at the end    \n",
    "    v = determine_action(params, y, w)\n",
    "    return cost_fn(y, v) \n",
    "\n",
    "# Action Loss\n",
    "def action_loss(actions, w, look_back, env, cost_fn = quad_loss):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    Args:\n",
    "    \"\"\"\n",
    "    y = np.zeros((env.n, 1))\n",
    "    for h in range(look_back, 1, -1):\n",
    "        y = env.dyn(y, actions[-h]) + w[-h-1]\n",
    "\n",
    "    # Don't update state at the end    \n",
    "    v = actions[-1]\n",
    "    return cost_fn(y, v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- History Updates ----------\n",
    "\n",
    "def update_noise(w, x, u, env):\n",
    "    w = jax.ops.index_update(w, 0, x - env.dyn(x, u))\n",
    "    w = np.roll(w, -1, axis = 0) \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class env():\n",
    "    \"\"\"\n",
    "    Description: The base, master LDS class that all other LDS subenvironments inherit. \n",
    "        Simulates a linear dynamical system with a lot of flexibility and variety in\n",
    "        terms of hyperparameter choices.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n, self.m, self.A, self.B = 2, 1, np.array([[1., 1.], [0., 1.]]), np.array([[0.], [1.]])\n",
    "        self.x = np.zeros((self.n,1))\n",
    "        self.dyn = lambda x, u: self.A @ x + self.B @ u\n",
    "        self.t = 0\n",
    "        \n",
    "    def step(self, u):\n",
    "        self.t += 1\n",
    "        return self.A @ self.x + self.B @ u + self.t % 7 // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Gradient Pertubation Controller\n",
    "\"\"\"\n",
    "\n",
    "import tigercontrol\n",
    "from tigercontrol.controllers import Controller\n",
    "from tigercontrol.controllers import LQR\n",
    "from tigercontrol.controllers.boosting.core import quad_loss, policy_loss\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import grad\n",
    "\n",
    "# GPC definition\n",
    "class GPC_v2(Controller):\n",
    "\n",
    "    def __init__(self, env, K = None, H = 3, look_back = 3, cost_fn = quad_loss, lr = 0.001):\n",
    "        \"\"\"\n",
    "        Description: Initialize the dynamics of the model\n",
    "        Args:\n",
    "            env (object): environment\n",
    "            K (float/numpy.ndarray): Starting policy (optional). \n",
    "            H (postive int): history of the controller \n",
    "            look_back (positive int): history (rollout) of the system \n",
    "            cost_fn (function): cost function\n",
    "            lr (float/numpy.ndarray): learning rate(s)\n",
    "        \"\"\"\n",
    "\n",
    "        self.n, self.m = env.n, env.m # State & Action Dimensions\n",
    "        self.env = env\n",
    "\n",
    "        self.t = 1 # Time Counter (for decaying learning rate)\n",
    "        self.lr, self.H = lr, H # Model Hyperparameters\n",
    "        self.look_back = look_back\n",
    "\n",
    "        # Model Parameters: initial linear policy / perturbation contributions\n",
    "        self.K = np.zeros((self.m, self.n)) if K is None else K\n",
    "        self.params = np.zeros((H, self.m, self.n))\n",
    "\n",
    "        # Past H + look_back noises\n",
    "        self.w = np.zeros((H + look_back, self.n, 1))\n",
    "\n",
    "        # past state and past action\n",
    "        self.x, self.u = np.zeros((self.n, 1)), np.zeros((self.m, 1))\n",
    "\n",
    "        self.determine_action = lambda params, x, w: -self.K @ x + \\\n",
    "                                        np.tensordot(params, w[-self.H:], axes = ([0, 2], [0, 1]))\n",
    "        \n",
    "        self.grad_policy = grad(policy_loss)\n",
    "\n",
    "    def update_params(self, grad = None, cost_fn = quad_loss):\n",
    "        \"\"\"\n",
    "        Description: Updates the parameters of the model\n",
    "        Args:\n",
    "            grad (float/numpy.ndarray): gradient of loss\n",
    "            cost_fn (function): current loss function\n",
    "            cost_val (float): current cost value\n",
    "        \"\"\"\n",
    "        # 1. Update t\n",
    "        self.t = self.t + 1 \n",
    "\n",
    "        # 2. Get gradients if not provided\n",
    "        delta_params = self.grad_policy(self.params, self.determine_action, self.w, \\\n",
    "                            self.look_back, self.env, cost_fn) if grad is None else grad\n",
    "\n",
    "        # 3. Execute parameter updates\n",
    "        self.params -= self.lr * delta_params\n",
    "\n",
    "    def update_history(self, x = None):\n",
    "        \"\"\"\n",
    "        Description: Updates the system history tracked by of the model\n",
    "        Args:\n",
    "            x (float/numpy.ndarray): observed state\n",
    "        \"\"\"\n",
    "        self.w = update_noise(self.w, x, self.u, self.env)\n",
    "        self.x = x\n",
    "        self.u = self.determine_action(self.params, x, self.w)\n",
    "\n",
    "    def get_action(self, x):\n",
    "        \"\"\"\n",
    "        Description: Return the action chosen by the controller for state x. No side-effects.\n",
    "        Args:\n",
    "            x (float/numpy.ndarray): system state\n",
    "        \"\"\"\n",
    "\n",
    "        return self.determine_action(self.params, x, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST GPC ###\n",
    "T = 50\n",
    "controller = GPC_v2(env)\n",
    "for t in range(T):\n",
    "    u = controller.get_action(env.x)\n",
    "    controller.update_params()\n",
    "    x = env.step(u)\n",
    "    controller.update_history(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tigercontrol\n",
    "from tigercontrol.controllers import Controller\n",
    "from tigercontrol.controllers.boosting.core import quad_loss, action_loss\n",
    "from tigercontrol.controllers.boosting.core import update_noise\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import grad\n",
    "\n",
    "class DynaBoost(Controller):\n",
    "    \"\"\"\n",
    "    Description: \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, N = 3, H = 3, cost_fn = quad_loss):\n",
    "        \"\"\"\n",
    "        Description: Initializes autoregressive controller parameters\n",
    "        Args:\n",
    "            controller_id (string): id of weak learner controller\n",
    "            controller_params (dict): dict of params to pass controller\n",
    "            N (int): default 3. Number of weak learners\n",
    "        \"\"\"\n",
    "        self.initialized = True\n",
    "\n",
    "        self.n, self.m = env.n, env.m # State & Action Dimensions\n",
    "        self.env = env # System\n",
    "\n",
    "        # 1. Maintain N copies of the algorithm \n",
    "        assert N > 0\n",
    "        self.N, self.H = N, H\n",
    "        self.controllers = []\n",
    "\n",
    "        #past state\n",
    "        self.x = np.zeros((self.n, 1))\n",
    "        # Past 2H noises\n",
    "        self.w = np.zeros((2 * H, self.n, 1))\n",
    "\n",
    "        # 2. Initialize the N weak learners\n",
    "        self.weak_controller = GPC_v2(env)\n",
    "        for _ in range(N):\n",
    "            new_controller = GPC_v2(env)\n",
    "            self.controllers.append(new_controller)\n",
    "\n",
    "        self.past_partial_actions = np.zeros((N+1, H, self.m, 1))\n",
    "\n",
    "        # Extract the set of actions of previous learners\n",
    "        def get_partial_actions(x):\n",
    "            u = np.zeros((self.N + 1, self.m, 1))\n",
    "            partial_u = np.zeros((self.m, 1))\n",
    "            for i, controller_i in enumerate(self.controllers):\n",
    "                eta_i = 2 / (i + 2)\n",
    "                pred_u = controller_i.get_action(x)\n",
    "                partial_u = (1 - eta_i) * partial_u + eta_i * pred_u\n",
    "                u = jax.ops.index_update(u, i + 1, partial_u)\n",
    "            return u\n",
    "\n",
    "        self.get_partial_actions = get_partial_actions\n",
    "\n",
    "        self.grad_action = grad(action_loss)\n",
    "\n",
    "        # Extract the set of actions of previous learners\n",
    "        def get_grads(partial_actions, w, cost_fn = quad_loss):\n",
    "            v_list = [self.grad_action(partial_actions[i], w, self.H, self.env, cost_fn) for i in range(self.N)]\n",
    "            return v_list\n",
    "        \n",
    "        self.get_grads = get_grads\n",
    "        \n",
    "        def linear_loss(controller_i_params, grad_i, w):\n",
    "            linear_loss_i = 0\n",
    "\n",
    "            y = np.zeros((n, 1))\n",
    "\n",
    "            for h in range(self.H):\n",
    "                v = self.weak_controller.determine_action(controller_i_params, y, w[:h+H])\n",
    "                linear_loss_i += np.dot(grad_i[h], v)\n",
    "                y = self.env.dyn(y, v) + w[h+H]\n",
    "\n",
    "            v = self.weak_controller.determine_action(controller_i_params, y, w[:h+H])\n",
    "            linear_loss_i += np.dot(grad_i[h], v)\n",
    "\n",
    "            return np.sum(linear_loss_i)\n",
    "        \n",
    "        self.grad_linear = grad(linear_loss)\n",
    "\n",
    "    def update_params(self, cost_fn = quad_loss, cost_val = None):\n",
    "        grads = self.get_grads(self.past_partial_actions, self.w, cost_fn)\n",
    "        for controller_i, grad_i in zip(self.controllers, grads):\n",
    "            controller_i.update_params(grad = self.grad_linear(controller_i.params, grad_i, self.w))\n",
    "\n",
    "    def update_history(self, x = None):\n",
    "        self.w = update_noise(self.w, x, self.past_partial_actions[-1][-1], self.env)\n",
    "        self.x = x\n",
    "        self.past_partial_actions = jax.ops.index_update(self.past_partial_actions,\\\n",
    "                                    jax.ops.index[:, 0], self.get_partial_actions(x))\n",
    "        self.past_partial_actions = np.roll(self.past_partial_actions, -1, axis = 1)\n",
    "\n",
    "    def get_action(self, x):\n",
    "        return self.get_partial_actions(x)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST DynaBoost ###\n",
    "T = 50\n",
    "controller = DynaBoost(env)\n",
    "for t in range(T):\n",
    "    u = controller.get_action(env.x)\n",
    "    controller.update_params()\n",
    "    x = env.step(u)\n",
    "    controller.update_history(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
